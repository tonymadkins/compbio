{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of splicing dataset = 10000\n"
     ]
    }
   ],
   "source": [
    "#Read splicing data\n",
    "\n",
    "df = pd.read_csv('Splicing_Data.txt', sep='\\t')\n",
    "df = df.loc[~np.isnan(df.SD1_Usage)].copy().reset_index(drop=True)\n",
    "\n",
    "df = df.iloc[-10000:].copy().reset_index(drop=True)\n",
    "\n",
    "df['Region1'] = df['Seqs'].str.slice(2, 37)\n",
    "\n",
    "print('Size of splicing dataset = ' + str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 6-mer features from sequence 0\n",
      "Extracting 6-mer features from sequence 2000\n",
      "Extracting 6-mer features from sequence 4000\n",
      "Extracting 6-mer features from sequence 6000\n",
      "Extracting 6-mer features from sequence 8000\n",
      "Shape of X = (10000, 4096)\n",
      "Shape of y = (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Generate 6mer feature matrix\n",
    "\n",
    "mer6_dict = {}\n",
    "mer6_list = []\n",
    "bases = list('ACGT')\n",
    "\n",
    "#Build dictionary of 6-mer -> index\n",
    "i = 0\n",
    "for b1 in bases :\n",
    "    for b2 in bases :\n",
    "        for b3 in bases :\n",
    "            for b4 in bases :\n",
    "                for b5 in bases :\n",
    "                    for b6 in bases :\n",
    "                        mer6_dict[b1 + b2 + b3 + b4 + b5 + b6] = i\n",
    "                        mer6_list.append(b1 + b2 + b3 + b4 + b5 + b6)\n",
    "                        i += 1\n",
    "\n",
    "#Loop over dataframe, fill matrix X with 6-mer counts\n",
    "X = sp.lil_matrix((len(df), len(mer6_dict)))\n",
    "for index, row in df.iterrows() :\n",
    "    if index % 2000 == 0 :\n",
    "        print('Extracting 6-mer features from sequence ' + str(index))\n",
    "    \n",
    "    region1 = row['Region1']\n",
    "    #Loop over all 6-mers in the current sequence\n",
    "    for j in range(0, len(region1) - 6 + 1) :\n",
    "        if region1[j:j+6] in mer6_dict :\n",
    "            #Increment X at the corresponding 6-mer index position\n",
    "            X[index, mer6_dict[region1[j:j+6]]] += 1.\n",
    "\n",
    "X = sp.csr_matrix(X)\n",
    "y = np.ravel(df['SD1_Usage'].values)\n",
    "\n",
    "print('Shape of X = ' + str(X.shape))\n",
    "print('Shape of y = ' + str(y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating logodds for 6-mer 0, AAAAAA, -0.10972614099450448\n",
      "Calculating logodds for 6-mer 1000, ATTGGA, -0.594009220241114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating logodds for 6-mer 2000, CTTCAA, 0.17423057558672736\n",
      "Calculating logodds for 6-mer 3000, GTGTGA, -0.2820610383732596\n",
      "Calculating logodds for 6-mer 4000, TTGGAA, -0.4075865029909619\n"
     ]
    }
   ],
   "source": [
    "#Problem 1.1\n",
    "#TODO: Calculate log odds ratios of 6-mers using feature matrix X and splicing ratios y\n",
    "\n",
    "X_col = sp.csc_matrix(X) #More efficient representation of X when working with columns\n",
    "logodds_ratios = np.zeros(X_col.shape[1])\n",
    "\n",
    "#Loop over every 6-mer index\n",
    "for j in range(logodds_ratios.shape[0]) :\n",
    "    j_nonzero = X_col[:,j].nonzero()[0]\n",
    "    mask = np.ones(X_col.shape[0])\n",
    "    mask[j_nonzero] = 0\n",
    "    j_zero = mask.nonzero()[0]\n",
    "    if len(j_nonzero) == 0:\n",
    "        y_has_j = 0\n",
    "    else:\n",
    "        y_has_j = np.sum(y[j_nonzero]) / len(j_nonzero)\n",
    "    y_not_j = np.sum(y[j_zero]) / len(j_zero)\n",
    "    \n",
    "    logodds_ratios[j] = np.log((y_has_j/(1 - y_has_j)) / (y_not_j/(1 - y_not_j)))\n",
    "    \n",
    "    if j % 1000 == 0 :\n",
    "        print(f'Calculating logodds for 6-mer {j}, {mer6_list[j]}, {logodds_ratios[j]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1.1\n",
    "#TODO: Plot the sorted Log odds ratios, and print the smallest and largest values\n",
    "odds_sort_index = np.argsort(logodds_ratios, axis=0)\n",
    "sorted_odds = logodds_ratios[odds_sort_index]\n",
    "sorted_mers = np.array(mer6_list)[odds_sort_index]\n",
    "print(f'minimum log odds : {sorted_mers[0]}, {sorted_odds[0]}')\n",
    "print(f'maximum log odds : {sorted_mers[len(sorted_mers) - 1]}, {sorted_odds[len(sorted_odds) - 1]}')\n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(12,8))\n",
    "plt.scatter(sorted_odds, sorted_mers)\n",
    "plt.title(\"Log odds ratios sorted\", fontsize=16)\n",
    "plt.xlabel('Log odds ratio', fontsize=14)\n",
    "plt.ylabel('6mer', fontsize=14)\n",
    "plt.yticks(np.arange(0, len(sorted_mers), step=256))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1.3\n",
    "#TODO: Split data (matrix X and vector y) into training and test sets. Test set should contain 2,000 data points\n",
    "split = 2000\n",
    "trainX = X[split:]\n",
    "testX = X[:split]\n",
    "trainY = y[split:]\n",
    "testY = y[:split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1.3\n",
    "#TODO: Implement Gradient Descent with KL-divergence gradients for regressing splice site usages\n",
    "\n",
    "#Helper function for computing log(x / y) in a safe way (whenever x or y is 0).\n",
    "def safe_kl_log(num, denom) :\n",
    "    log_vec = np.zeros(num.shape)\n",
    "    log_vec[(num > 0) & (denom > 0)] = np.log(num[(num > 0) & (denom > 0)] / denom[(num > 0) & (denom > 0)])\n",
    "    \n",
    "    return log_vec\n",
    "\n",
    "#Compute the KL divergence loss (alpha is regularization parameter)\n",
    "def kl_divergence_loss(X, w, w_0, y_true, alpha=0.0001) :\n",
    "    #TODO: Implement and return kl divergence loss function L(w, w0, alpha)\n",
    "    y_guesses = np.zero(y_true)\n",
    "    for i in range(X.shape[0]):\n",
    "        tmp = w0 + np.sum(w * X[i])\n",
    "        y_guesses[i] = 1 / (1 + (e**-tmp))\n",
    "    num_sum = np.sum(y_true * safe_kl_log(y_true, y_guesses) + (np.ones(y_true.shape) - y_true) * safe_kl_log(np.ones(y_true.shape) - ytrue, np.ones(y_true.shape) - y_guesses))\n",
    "    num_sum2 = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        num_sum2 += y_true[i] * safe_kl_log(y_true[i], y_guesses[i]) + (1 - y_true[i]) * safe_kl_log(1 - y_true[i], 1 - y_guesses[i])\n",
    "    print(f'{num_sum} == {num_sum2}')\n",
    "    L = num_sum / len(y_guesses) + alpha * np.sum([wj ** 2 for wj in w]) / 2\n",
    "    return L\n",
    "\n",
    "#Compute the KL divergence gradients for the weight vector w and intercept term w_0 (alpha is regularization parameter)\n",
    "def kl_divergence_gradients(X, w, w_0, y_true, alpha=0.0001) :\n",
    "    #TODO: Implement and return kl divergence loss gradients for w and w_0\n",
    "    y_guesses = np.zero(y_true)\n",
    "    for i in range(X.shape[0]):\n",
    "        tmp = w0 + np.sum(w * X[i])\n",
    "        y_guesses[i] = 1 / (1 + (e**-tmp))\n",
    "    w_grad = np.zeros(w.shape)\n",
    "    for w_k in range(len(w)):\n",
    "        w_grad[w_k] = np.sum(X[:,w_k] * (y_guesses - y_true)) / len(y_true) + alpha * w[w_k]\n",
    "    w0_grad = np.sum(y_guesses - y_true) / len(y_true)\n",
    "    \n",
    "    return w_grad, w_0\n",
    "\n",
    "#Gradient Descent algorithm to optimize weights w and w_0\n",
    "def gradient_descent(X_train, y_train, X_test, y_test, w, w_0, step_size=0.1, alpha=0.0001, max_epochs=2000) :\n",
    "    \n",
    "    mean_train_losses = []\n",
    "    mean_test_losses = []\n",
    "    for epoch in range(max_epochs) : #Stop after unreasonable # of iterations, in case we never converge\n",
    "        if epoch % 50 == 1 and len(mean_train_losses) > 0 :\n",
    "            print('Training epoch = ' + str(epoch))\n",
    "            print('Training set KL-div = ' + str(round(mean_train_losses[-1], 4)))\n",
    "            print('Test set KL-div = ' + str(round(mean_test_losses[-1], 4)))\n",
    "        \n",
    "        #TODO: Calculate the KL loss and gradients on the training set\n",
    "        kl_loss_train = kl_divergence_loss(X_train, w, w_0, y_train)\n",
    "        w_grad, w0_grad = kl_divergence_gradients(X_train, w, w_0, y_train)\n",
    "        \n",
    "        #Update your weights w and w_0 based on the gradients\n",
    "        w = w - step_size * w_grad\n",
    "        w_0 = w_0 - step_size * w0_grad\n",
    "        \n",
    "        #Append your mean train and test loss to 'mean_train_losses' and 'mean_test_losses'\n",
    "        kl_loss_test = kl_divergence_loss(X_test, w, w_0, y_test)\n",
    "        mean_train_losses.append(kl_loss_train)\n",
    "        mean_test_losses.append(kl_loss_test)\n",
    "        \n",
    "        #TODO: Stop the loop once the training loss stops decreasing significantly\n",
    "        if mean_train_losses[-2] - mean_train_losses[-1] < alpha:\n",
    "            break\n",
    "\n",
    "    print('Gradient descent completed.')\n",
    "    print('Final training set KL-div = ' + str(round(mean_train_losses[-1], 4)))\n",
    "    print('Final test set KL-div = ' + str(round(mean_test_losses[-1], 4)))\n",
    "    \n",
    "    return w, w_0, mean_train_losses, mean_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1.3\n",
    "#TODO: Plot training and test set loss (mean KL-div) vs. training iteration\n",
    "w = np.zeros()\n",
    "w_0 = 0\n",
    "w, w_0, mean_train_losses, mean_test_losses = gradient_descent(trainX, trainY, testX, testY, w, w0)\n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(12,8))\n",
    "plt.scatter(range(len(mean_test_losses)), mean_test_losses)\n",
    "plt.title(\"Test set loss vs. training iteration\", fontsize=16)\n",
    "plt.xlabel('Training iteration', fontsize=14)\n",
    "plt.ylabel('Test set loss', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#TODO: Scatter plot of true vs. pred SD1 usage on test set, and print R^2 coefficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1.3\n",
    "#TODO: Plot the 10 6-mers and corresponding weights of largest magnitude\n",
    "#TODO: Plot the 10 6-mers and corresponding weights of smallest magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
